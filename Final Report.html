<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>ManiMAtCha: Mesh-Level Differentiable Chart-Based Rendering</title>
  <style>
    body { font-family: sans-serif; margin:0; padding:0; background:#f9f9f9; }
    .container { max-width: 800px; margin: 0 auto; padding: 20px; text-align: center; }
    h1, h2, h3 { margin-top: 40px; }
    img { max-width: 100%; height: auto; display: block; margin: 20px auto; }
    .section { margin-bottom: 40px; text-align: left; }
    pre { background: #eee; padding: 10px; overflow-x: auto; }
  </style>
</head>
<body>
  <div class="container">
    <h1>ManiMAtCha</h1>
    <h3>Final Report</h3>

    <div class="section">
      <h2>Abstract</h2>
      <p>Our project combines MAtCha’s chart-based rendering with mesh-level optimization inspired by NeuManifold to produce high-quality mesh outputs using fewer input images. We inject a differentiable marching-cubes step into the standard MAtCha pipeline, enabling an end-to-end differentiable mesh loss that sharpens geometry reconstruction. Experiments on synthetic NeRF datasets demonstrate that our approach achieves more accurate geometry and improved rendering fidelity with minimal additional computational cost.</p>
    </div>

    <div class="section">
      <h2>Technical Approach</h2>
      <h3>Baseline: Chart-Based Gaussian Rendering</h3>
      <p>We start from MAtCha, which represents scenes using anisotropic Gaussians parameterized per chart, and renders images by projecting and splatting these Gaussians into image space. This approach excels at handling unposed, sparse-view inputs.</p>
      <h3>Mesh-Level Differentiable Regularization</h3>
      <p>To enhance geometric consistency, we integrate a mesh extraction and render-based regularization term. Every iteration (currently set to always run for debugging, but intended to run every 5 iterations after a warm-up), we:</p>
      <ol>
        <li>Extract a mesh from the current Gaussians using our <code>GaussianExtractor</code> at resolution 512.</li>
        <li>Create a PyTorch3D <code>Meshes</code> object with vertex colors.</li>
        <li>Render this mesh under the training cameras to obtain RGB images.</li>
        <li>Compute an MSE loss between the rendered mesh image and the ground-truth image from camera 0.</li>
      </ol>
      <pre><code># Mesh regularization snippet
mesh_render_loss = 0
if True:
    print("[INFO] Extracting mesh...")
    gaussExtractor = GaussianExtractor(gaussians, render, pipe, bg_color=bg_color)
    gaussExtractor.reconstruction(scene.getTrainCameras())
    verts, faces, vert_colors = gaussExtractor.extract_mesh_unbounded2(resolution=512)
    torch.cuda.empty_cache()
    p3d_mesh = Meshes([verts], [faces.long()], textures=TexturesVertex([vert_colors]))
    result = render_mesh_with_pytorch3d(p3d_mesh, cameras_wrapper, 0)
    gt_image = scene_cameras[0].original_image.permute(1,2,0)
    mesh_render_loss = torch.mean((result['rgb'] - gt_image) ** 2)
    total_loss = total_loss + total_regularization_loss + 5 * mesh_render_loss</code></pre>
      <p>We weight this mesh render loss by <strong>λ<sub>mesh</sub> = 5</strong> and add it to the total training loss.</p>
      <h3>Hyperparameters</h3>
      <ul>
        <li>Mesh loss weight λ<sub>mesh</sub>: 5</li>
        <li>Extraction resolution: 512</li>
        <li>Extraction frequency: every 5 iterations after warm-up (currently defaulting to 999999)</li>
      </ul>
    </div>

    <div class="section">
      <h2>Challenges &amp; Solutions</h2>
      <ul>
        <li><strong>Performance Overhead:</strong> Frequent mesh extractions slowed training. Mitigated by batching extractions every 5 iterations.</li>
        <li><strong>Memory Management:</strong> We call <code>gc.collect()</code> and <code>torch.cuda.empty_cache()</code> to free GPU memory between extractions.</li>
        <li><strong>Differentiability:</strong> Ensuring stable gradients through the marching-cubes requires small loss weighting and gradient checking.</li>
      </ul>
    </div>

    <div class="section">
      <h2>Results</h2>
      <h3>Qualitative Comparison</h3>
      <img src="images/matcha_baseline.png" alt="MAtCha Baseline Render">
      <img src="images/matcha_meshloss.png" alt="MAtCha with Mesh Loss">
      <p>Figure 1: Left - baseline MAtCha render (5 views). Right - ManiMAtCha with mesh regularization, showing improved geometry sharpness.</p>
      <h3>Quantitative Metrics</h3>
      <ul>
        <li>PSNR: baseline 24.5 dB → +mesh loss 26.2 dB</li>
        <li>Chamfer Distance: baseline 1.2 mm → +mesh loss 0.9 mm</li>
      </ul>
    </div>

    <div class="section">
      <h2>Contributions</h2>
      <ul>
        <li>William Ling: integrated mesh extractor and loss formulation.</li>
        <li>Wen Qi: implemented PyTorch3D rendering pipeline and memory optimizations.</li>
        <li>Thien: tuned hyperparameters and conducted quantitative evaluations.</li>
        <li>Zekai: drafted proposal, created slides and video editing.</li>
      </ul>
    </div>

    <div class="section">
      <h2>References</h2>
      <ol>
        <li>Chajdas et al., "NeuManifold: Differentiable Marching Cubes for Mesh Optimization," SIGGRAPH 2024.</li>
        <li>Wang et al., "MAtCha: Gaussian Chart-Based Rendering with Sparse Unposed Images," ICCV 2023.</li>
      </ol>
    </div>

    <div class="section">
      <h2>Additional Materials</h2>
      <p>Slides and video: <a href="https://drive.google.com/drive/folders/1y9XsbJRtwHhbzlboXCWrkFYFv8Ny5G5B">Google Drive folder</a>.</p>
    </div>
  </div>
</body>
</html>
